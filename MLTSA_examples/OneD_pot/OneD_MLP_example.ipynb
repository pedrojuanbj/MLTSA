{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Tensorflow training 1D potential example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Since this is a bit more advanced than the sklearn we recommend you start the other one first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"First we import our dataset examples\"\"\"\n",
    "from OneD_pot_data import potentials\n",
    "from OneD_pot_data import dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#This sets the potentials, don't re-run\n",
    "total_n_pots = 25\n",
    "n_DW = 5\n",
    "relevant_DW_n = 2\n",
    "#After defining the desired parameters we define the potentials accordingly\n",
    "pots = potentials(total_n_pots, n_DW, relevant_DW_n)\n",
    "# This creates the first dataset of data.\n",
    "# It creates the mixing coefficients don't re-run\n",
    "n_features = 180\n",
    "degree_of_mixing = 2\n",
    "#We specified the number of features wanted and how much they will mix\n",
    "oneD_dataset = dataset(pots, n_features, degree_of_mixing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "This has set up our dataset for further use, since TensorFlow is more scalable and compatible with GPU calculations, we will do a more extensive search on this example. \n\nLet's generate the actual linear mixed data we will use for training.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Generate the trajectories\n",
    "n_simulations = 100\n",
    "n_steps = 500\n",
    "data, ans = oneD_dataset.generate_linear(n_simulations, n_steps)\n",
    "data_val, ans_val = oneD_dataset.generate_linear(n_simulations/2, n_steps)\n",
    "\n",
    "#Prepare it for training\n",
    "time_frame = [30, 60] #Same time frame as the sklearn one\n",
    "X, Y = oneD_dataset.PrepareData(data, ans, time_frame, mode=\"Normal\")\n",
    "X_val, Y_val = oneD_dataset.PrepareData(data_val, ans_val, time_frame, mode=\"Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "\nNow that we have got the X and Y ready to fit to, we will train the model, in this example we want to train a Multi-Layer Perceptron just like the one in Sklearn, but we will use TensorFlow instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Luckily we have an MLP set up to use\n",
    "from MLTSA_tensorflow import MLP_tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Let's do the splits to validate our training\n",
    "# data = [x_train, x_test, y_train, y_test]\n",
    "epoch_data = train_test_split(X, Y, test_size=0.4, stratify=Y) # We'll train on 40% of the data\n",
    "\n",
    "acc_train, acc_test, acc_val, loss = MLP_tf.tf_train(data=epoch_data, validation=[X_val, Y_val], mode=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "\nThe previous training has generated a record of the training, test and validation accuracy as well as the evolution of the loss throughout the epochs. \n\nThese can be plotted to see how the model is learning the outcome. Additionally, a checkpoint of our trained model has been created and saved for us at \"model_best_accuracy.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(acc_train*100, color=\"r\", label=\"Train\")\n",
    "ax[0].plot(acc_test*100, color=\"b\", label=\"Test\")\n",
    "ax[0].plot(acc_val*100, color=\"g\", label=\"Validation\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(loss, label=\"training loss\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can move onto trying the MLTSA for this ML model. For that we call the MLTSA() method within the tensorflow package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "source": "",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}